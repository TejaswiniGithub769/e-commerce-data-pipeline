# e-commerce-data-pipeline

In this project, data engineering pipeline utilizing Apache Spark, Azure Databricks, and Data Build Tool (DBT), with Azure serving as our cloud provider. The primary objective was to streamline the process of data ingestion into the lakehouse architecture, ensuring that data from various sources could be collected, stored, and managed efficiently. By leveraging Azure Data Factory (ADF) for data integration, we were able to automate and orchestrate the data workflows, enabling seamless movement and transformation of data across different stages of the pipeline. This setup provided a robust foundation for handling large volumes of data, ensuring that the ingestion process was scalable, reliable, and secure.

The project further illustrates the transformation and enrichment of data within Azure Databricks, where Apache Spark is employed for its powerful distributed processing capabilities. Databricks' collaborative environment facilitated the development, testing, and deployment of data transformation pipelines. Additionally, DBT was integrated to manage data transformations and modeling, enabling us to apply medallion architecture principles effectively. This approach involved organizing data into layers â€“ bronze for raw data, silver for cleaned and enriched data, and gold for business-ready data. The medallion architecture ensured that data was systematically refined at each stage, improving data quality and accessibility for analytics and reporting. Overall, this project showcases the integration of modern data engineering tools and methodologies to create a comprehensive, scalable, and efficient data pipeline on the Azure platform
